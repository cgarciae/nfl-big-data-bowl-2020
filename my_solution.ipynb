{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â NFL Big Data Bowl 2020 - My Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows my own approach to the NFL Big Data Bowl 2020 competition. As the winner solution, our model is a convolutional network [1]. However, we exploit the concept of **_player influence area_** (Competition VIP Hint [2]) which was not taken into account for the majority of the winner solutions [3].\n",
    "\n",
    "The **_player influence area_** is a concept introduced in [2] which characterized the influence that a player has over each point on the field as a function of 3 variables: player velocity, player position and ball position. The next figure illustrates this concept in a particular soccer play.\n",
    "\n",
    "<img src=\"images/player_influence_area.gif\" style=\"width:680px;height:340px;\">\n",
    "\n",
    "In this approach, we construct the input tensor to the convolutional network as an image around the rushing play, where each channel is the **_player influence area_** of each player. The next figure illustrates the idea behind the input tensor.\n",
    "\n",
    "<img src=\"images/input_tensor.png\" style=\"width:680px;height:240px;\">\n",
    "\n",
    "Similar to the notebook in [1], the remainder of this notebook is organized as follows. Section 1 contains the code for data processing and data augmentation. Section 2 provides the model structure. Finally, section 3 draws some conclusions and some possible improvements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import utils\n",
    "from pitch_control import compute_player_influence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', dtype={'WindSpeed': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows/plays in df:  31007\n"
     ]
    }
   ],
   "source": [
    "def split_play_and_player_cols(df, predicting=False):\n",
    "    df['IsRusher'] = df['NflId'] == df['NflIdRusher']\n",
    "    \n",
    "    df['PlayId'] = df['PlayId'].astype(str)\n",
    "    \n",
    "    # We must assume here that the first 22 rows correspond to the same player:\n",
    "    player_cols = [\n",
    "        'PlayId', # This is the link between them\n",
    "        'Season',\n",
    "        'Team',\n",
    "        'X',\n",
    "        'Y',\n",
    "        'S',\n",
    "        'Dis',\n",
    "        'Dir',\n",
    "        'NflId',\n",
    "        'IsRusher',\n",
    "    ]\n",
    "\n",
    "    df_players = df[player_cols]\n",
    "    \n",
    "    play_cols = [\n",
    "        'PlayId',\n",
    "        'Season',\n",
    "        'PossessionTeam',\n",
    "        'HomeTeamAbbr',\n",
    "        'VisitorTeamAbbr',\n",
    "        'PlayDirection', \n",
    "        'FieldPosition',\n",
    "        'YardLine',\n",
    "    ]\n",
    "    \n",
    "    if not predicting:\n",
    "        play_cols.append('Yards')\n",
    "        \n",
    "    df_play = df[play_cols].copy()\n",
    "\n",
    "    ## Fillna in FieldPosition attribute\n",
    "    #df['FieldPosition'] = df.groupby(['PlayId'], sort=False)['FieldPosition'].apply(lambda x: x.ffill().bfill())\n",
    "    \n",
    "    # Get first \n",
    "    df_play = df_play.groupby('PlayId').first().reset_index()\n",
    "\n",
    "    print('rows/plays in df: ', len(df_play))\n",
    "    assert df_play.PlayId.nunique() == df.PlayId.nunique(), \"Play/player split failed?\"  # Boom\n",
    "    \n",
    "    return df_play, df_players\n",
    "\n",
    "play_ids = train[\"PlayId\"].unique()\n",
    "\n",
    "df_play, df_players = split_play_and_player_cols(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_team_abbr(df):\n",
    "\n",
    "    #These are only problems:\n",
    "    map_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\n",
    "    for abb in train['PossessionTeam'].unique():\n",
    "        map_abbr[abb] = abb\n",
    "\n",
    "    df['PossessionTeam'] = df['PossessionTeam'].map(map_abbr)\n",
    "    df['HomeTeamAbbr'] = df['HomeTeamAbbr'].map(map_abbr)\n",
    "    df['VisitorTeamAbbr'] = df['VisitorTeamAbbr'].map(map_abbr)\n",
    "\n",
    "    df['HomePossession'] = df['PossessionTeam'] == df['HomeTeamAbbr']\n",
    "    \n",
    "    return\n",
    "\n",
    "process_team_abbr(df_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_play_direction(df):\n",
    "    df['IsPlayLeftToRight'] = df['PlayDirection'].apply(lambda val: True if val.strip() == 'right' else False)\n",
    "    return\n",
    "\n",
    "process_play_direction(df_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_yard_til_end_zone(df):\n",
    "    def convert_to_yardline100(row):\n",
    "        return (100 - row['YardLine']) if (row['PossessionTeam'] == row['FieldPosition']) else row['YardLine']\n",
    "    df['Yardline100'] = df.apply(convert_to_yardline100, axis=1)\n",
    "    return\n",
    "\n",
    "process_yard_til_end_zone(df_play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players = df_players.merge(\n",
    "    df_play[['PlayId', 'PossessionTeam', 'HomeTeamAbbr', 'PlayDirection', 'Yardline100']], \n",
    "    how='left', on='PlayId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players.loc[df_players.Season == 2017, 'S'] = 10*df_players.loc[df_players.Season == 2017,'Dis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_direction(df):\n",
    "    # adjusted the data to always be from left to right\n",
    "    df['HomePossesion'] = df['PossessionTeam'] == df['HomeTeamAbbr']\n",
    "\n",
    "    df['Dir_rad'] = np.mod(90 - df.Dir, 360) * math.pi/180.0\n",
    "\n",
    "    df['ToLeft'] = df.PlayDirection == \"left\"\n",
    "    df['TeamOnOffense'] = \"home\"\n",
    "    df.loc[df.PossessionTeam != df.HomeTeamAbbr, 'TeamOnOffense'] = \"away\"\n",
    "    df['IsOnOffense'] = df.Team == df.TeamOnOffense # Is player on offense?\n",
    "    df['X_std'] = df.X\n",
    "    df.loc[df.ToLeft, 'X_std'] = 120 - df.loc[df.ToLeft, 'X']\n",
    "    df['Y_std'] = df.Y\n",
    "    df.loc[df.ToLeft, 'Y_std'] = 160/3 - df.loc[df.ToLeft, 'Y']\n",
    "    df['Dir_std'] = df.Dir_rad\n",
    "    df.loc[df.ToLeft, 'Dir_std'] = np.mod(np.pi + df.loc[df.ToLeft, 'Dir_rad'], 2*np.pi)\n",
    "   \n",
    "    #Replace Null in Dir_rad\n",
    "    df.loc[(df.IsOnOffense) & df['Dir_std'].isna(),'Dir_std'] = 0.0\n",
    "    df.loc[~(df.IsOnOffense) & df['Dir_std'].isna(),'Dir_std'] = np.pi\n",
    "\n",
    "standarize_direction(df_players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "For training, we assume that in a mirrored world the runs would have had the same outcomes. We apply 50% augmentation to flip the Y coordinates (and all respective relative features emerging from it). Furthermore, the function process_tracking_data computes the projections on X and Y for the velocity of each player and other features relative to rusher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(df, sample_ids):\n",
    "    df_sample = df.loc[df.PlayId.isin(sample_ids)].copy()\n",
    "    df_sample['Y_std'] = 160/3  - df_sample['Y_std']\n",
    "    df_sample['Dir_std'] = df_sample['Dir_std'].apply(lambda x: 2*np.pi - x)\n",
    "    df_sample['PlayId'] = df_sample['PlayId'].apply(lambda x: x+'_aug')\n",
    "    return df_sample\n",
    "\n",
    "\n",
    "sample_ids = np.random.choice(df_play.PlayId.unique(), int(0.5*len(df_play.PlayId.unique())))\n",
    "\n",
    "df_players_aug = data_augmentation(df_players, sample_ids)\n",
    "df_players = pd.concat([df_players, df_players_aug])\n",
    "df_players.reset_index()\n",
    "\n",
    "df_play_aug = df_play.loc[df_play.PlayId.isin(sample_ids)].copy()\n",
    "df_play_aug['PlayId'] = df_play_aug['PlayId'].apply(lambda x: x+'_aug')\n",
    "df_play = pd.concat([df_play, df_play_aug])\n",
    "df_play.reset_index()\n",
    "\n",
    "# This is necessary to maintain the order when in the next cell we use groupby\n",
    "df_players.sort_values(by=['PlayId'],inplace=True)\n",
    "df_play.sort_values(by=['PlayId'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any null values:  0\n"
     ]
    }
   ],
   "source": [
    "tracking_level_features = [\n",
    "    'PlayId',\n",
    "    'IsOnOffense',\n",
    "    'X_std',\n",
    "    'Y_std',\n",
    "    'S',\n",
    "    'Dir_std',\n",
    "    'IsRusher'\n",
    "]\n",
    "\n",
    "df_all_feats = df_players[tracking_level_features]\n",
    "\n",
    "print('Any null values: ', df_all_feats.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PlayId</th>\n",
       "      <th>IsOnOffense</th>\n",
       "      <th>X_std</th>\n",
       "      <th>Y_std</th>\n",
       "      <th>S</th>\n",
       "      <th>Dir_std</th>\n",
       "      <th>IsRusher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170907000118</td>\n",
       "      <td>False</td>\n",
       "      <td>46.09</td>\n",
       "      <td>18.493333</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.620015</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20170907000118</td>\n",
       "      <td>True</td>\n",
       "      <td>45.42</td>\n",
       "      <td>24.863333</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.250106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20170907000118</td>\n",
       "      <td>True</td>\n",
       "      <td>45.42</td>\n",
       "      <td>24.213333</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.487820</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20170907000118</td>\n",
       "      <td>True</td>\n",
       "      <td>45.40</td>\n",
       "      <td>21.453333</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.046775</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20170907000118</td>\n",
       "      <td>True</td>\n",
       "      <td>41.25</td>\n",
       "      <td>22.803333</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.423417</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682150</th>\n",
       "      <td>20191125003789</td>\n",
       "      <td>False</td>\n",
       "      <td>47.84</td>\n",
       "      <td>28.243333</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.377561</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682151</th>\n",
       "      <td>20191125003789</td>\n",
       "      <td>False</td>\n",
       "      <td>47.77</td>\n",
       "      <td>21.383333</td>\n",
       "      <td>1.75</td>\n",
       "      <td>3.137055</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682152</th>\n",
       "      <td>20191125003789</td>\n",
       "      <td>False</td>\n",
       "      <td>47.92</td>\n",
       "      <td>26.593333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.868697</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682144</th>\n",
       "      <td>20191125003789</td>\n",
       "      <td>False</td>\n",
       "      <td>48.53</td>\n",
       "      <td>23.733333</td>\n",
       "      <td>0.71</td>\n",
       "      <td>4.232249</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682143</th>\n",
       "      <td>20191125003789</td>\n",
       "      <td>False</td>\n",
       "      <td>58.73</td>\n",
       "      <td>23.263333</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.389604</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>950708 rows Ã 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                PlayId  IsOnOffense  X_std      Y_std     S   Dir_std  \\\n",
       "0       20170907000118        False  46.09  18.493333  4.00  1.620015   \n",
       "21      20170907000118         True  45.42  24.863333  2.40  0.250106   \n",
       "20      20170907000118         True  45.42  24.213333  2.20  0.487820   \n",
       "19      20170907000118         True  45.40  21.453333  1.70  0.046775   \n",
       "18      20170907000118         True  41.25  22.803333  3.80  0.423417   \n",
       "...                ...          ...    ...        ...   ...       ...   \n",
       "682150  20191125003789        False  47.84  28.243333  1.01  3.377561   \n",
       "682151  20191125003789        False  47.77  21.383333  1.75  3.137055   \n",
       "682152  20191125003789        False  47.92  26.593333  0.70  3.868697   \n",
       "682144  20191125003789        False  48.53  23.733333  0.71  4.232249   \n",
       "682143  20191125003789        False  58.73  23.263333  3.48  3.389604   \n",
       "\n",
       "        IsRusher  \n",
       "0          False  \n",
       "21         False  \n",
       "20         False  \n",
       "19         False  \n",
       "18          True  \n",
       "...          ...  \n",
       "682150     False  \n",
       "682151     False  \n",
       "682152     False  \n",
       "682144     False  \n",
       "682143     False  \n",
       "\n",
       "[950708 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the train tensor to feed the convolutional network. The following image depicts the structure of the input tensor:\n",
    "\n",
    "<img src=\"images/input_tensor.png\" style=\"width:680px;height:240px;\">\n",
    "\n",
    "Note that the input image contain 22 channels, each channel represents the **_player influence area_** of a player\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 43214/43214 [6:50:26<00:00,  1.75it/s]\n",
      "CPU times: user 12h 25s, sys: 1h 24min 53s, total: 13h 25min 18s\n",
      "Wall time: 6h 50min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "grouped = df_all_feats.groupby('PlayId')\n",
    "# channel first\n",
    "train_x = np.zeros([len(grouped.size()),22,30,30])\n",
    "i = 0\n",
    "play_ids = df_play.PlayId.values\n",
    "for name, group in tqdm(grouped):\n",
    "    \n",
    "    [[rusher_x, rusher_y, rusher_S, rusher_Dir]] = group.loc[group.IsRusher==1,['X_std', 'Y_std','S','Dir_std']].values\n",
    "    ball_coords = [rusher_x, rusher_y]\n",
    "    player_coords = [rusher_x, rusher_y]\n",
    "\n",
    "    x = np.linspace(int(rusher_x), int(rusher_x) + 14, 30)\n",
    "    y = np.linspace(17, 46, 30)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    train_x[i,0] = compute_player_influence(X, Y, rusher_Dir, rusher_S, player_coords, ball_coords)\n",
    "\n",
    "    offense_ids = group[group.IsOnOffense & ~group.IsRusher].index\n",
    "    defense_ids = group[~group.IsOnOffense].index\n",
    "\n",
    "    for j, offense_id in enumerate(offense_ids):\n",
    "        [player_x, player_y, player_S, player_Dir] = group.loc[offense_id,['X_std', 'Y_std','S','Dir_std']].values\n",
    "        player_coords = [player_x, player_y]\n",
    "        train_x[i,j+1] = compute_player_influence(X, Y, player_Dir, player_S, player_coords, ball_coords)\n",
    "    \n",
    "    for j, defense_id in enumerate(defense_ids):\n",
    "        [player_x, player_y, player_S, player_Dir] = group.loc[defense_id,['X_std', 'Y_std','S','Dir_std']].values\n",
    "        player_coords = [player_x, player_y]\n",
    "        train_x[i,j+11] = compute_player_influence(X, Y, player_Dir, player_S, player_coords, ball_coords)\n",
    "    i+=1\n",
    "\n",
    "np.save('data_pitch_control/train_x(augmented-50).npy', train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max yardIndex:  198\n",
      "max yardIndexClipped:  150\n",
      "min yardIndex:  84\n",
      "min yardIndexClipped:  84\n"
     ]
    }
   ],
   "source": [
    "# Transform Y into indexed-classes:\n",
    "train_y = df_play[['PlayId', 'Yards']].copy()\n",
    "\n",
    "train_y['YardIndex'] = train_y['Yards'].apply(lambda val: val + 99)\n",
    "\n",
    "min_idx_y = 71\n",
    "max_idx_y = 150\n",
    "\n",
    "train_y['YardIndexClipped'] = train_y['YardIndex'].apply(\n",
    "    lambda val: min_idx_y if val < min_idx_y else max_idx_y if val > max_idx_y else val)\n",
    "\n",
    "print('max yardIndex: ', train_y.YardIndex.max())\n",
    "print('max yardIndexClipped: ', train_y.YardIndexClipped.max())\n",
    "print('min yardIndex: ', train_y.YardIndex.min())\n",
    "print('min yardIndexClipped: ', train_y.YardIndexClipped.min())\n",
    "\n",
    "train_y.to_pickle('data_pitch_control/train_y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season = df_play[['PlayId', 'Season']].copy()\n",
    "df_season.to_pickle('data_pitch_control/df_season.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure (ConvNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43214, 30, 30, 22)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = np.load('data_pitch_control/train_x(augmented-50).npy') \n",
    "train_y = pd.read_pickle('data_pitch_control/train_y.pkl') \n",
    "df_season = pd.read_pickle('data_pitch_control/df_season.pkl')\n",
    "\n",
    "#num_classes_y = 199\n",
    "min_idx_y = 71\n",
    "max_idx_y = 150\n",
    "num_classes_y = max_idx_y - min_idx_y + 1\n",
    "\n",
    "# Define deffense values as negatives\n",
    "train_x[:,11:] = -train_x[:,11:]\n",
    "\n",
    "# Transpose to set the input as \"channel_last\"\n",
    "train_x = np.transpose(train_x, (0,2,3,1))\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, AvgPool2D, Flatten,\n",
    "    Input, BatchNormalization, Dense, Add, Lambda, Dropout, LayerNormalization)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "\n",
    "import tensorflow as tf \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def crps(y_true, y_pred):\n",
    "    loss = K.mean(K.sum((K.cumsum(y_pred, axis = 1) - K.cumsum(y_true, axis=1))**2, axis=1))/199\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network architecture proposed in this approach follows the idea of classic convolutional networks (Le-Net, AlexNet or VGG-16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        736       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 80)                10320     \n",
      "=================================================================\n",
      "Total params: 175,952\n",
      "Trainable params: 175,184\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_conv_net(num_classes_y):\n",
    "    #_, x, y, z = train_x.shape\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional Layera\n",
    "    model.add(Conv2D(32, input_shape=(30,30,22), kernel_size=(1,1), strides=(1,1), padding=\"same\", activation = \"relu\"))\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation = \"relu\"))\n",
    "\n",
    "    # Max Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"))\n",
    "\n",
    "    # Convolutional Layer\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), strides=(1,1), padding=\"valid\", activation = \"relu\"))\n",
    "    \n",
    "    # Max Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\"))\n",
    "\n",
    "    # Convolutional Layer\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), strides=(1,1), padding=\"valid\", activation = \"relu\"))\n",
    "    \n",
    "    # Max Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\"))\n",
    "\n",
    "    # Passing it to a Fully Connected layer\n",
    "    model.add(Flatten())\n",
    "    # 1st Fully Connected Layer\n",
    "    model.add(Dense(256, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 2nd Fully Connected Layer\n",
    "    model.add(Dense(128, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(num_classes_y, activation='softmax')) \n",
    "    return model\n",
    "\n",
    "model = get_conv_net(num_classes_y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(Callback):\n",
    "    def __init__(self, model, callbacks, data):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.callbacks = callbacks\n",
    "        self.data = data\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_begin(logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_end(logs)\n",
    "\n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        X_valid, y_valid = self.data[0], self.data[1]\n",
    "\n",
    "        y_pred = self.model.predict(X_valid)\n",
    "        y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n",
    "        y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
    "        val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * X_valid.shape[0])\n",
    "        logs['val_CRPS'] = val_s\n",
    "        \n",
    "        for callback in self.callbacks:\n",
    "            callback.on_epoch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Train on 37812 samples, validate on 3270 samples\n",
      "Epoch 1/20\n",
      "37812/37812 [==============================] - 59s 2ms/sample - loss: 0.0159 - val_loss: 0.0141\n",
      "Epoch 2/20\n",
      "37812/37812 [==============================] - 52s 1ms/sample - loss: 0.0141 - val_loss: 0.0139\n",
      "Epoch 3/20\n",
      "37812/37812 [==============================] - 51s 1ms/sample - loss: 0.0137 - val_loss: 0.0137\n",
      "Epoch 4/20\n",
      "37812/37812 [==============================] - 51s 1ms/sample - loss: 0.0134 - val_loss: 0.0154\n",
      "Epoch 5/20\n",
      "37812/37812 [==============================] - 52s 1ms/sample - loss: 0.0132 - val_loss: 0.0137\n",
      "Epoch 6/20\n",
      "37812/37812 [==============================] - 53s 1ms/sample - loss: 0.0131 - val_loss: 0.0133\n",
      "Epoch 7/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0130 - val_loss: 0.0131\n",
      "Epoch 8/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0129 - val_loss: 0.0134\n",
      "Epoch 9/20\n",
      "37812/37812 [==============================] - 53s 1ms/sample - loss: 0.0129 - val_loss: 0.0132\n",
      "Epoch 10/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0128 - val_loss: 0.0130\n",
      "Epoch 11/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0127 - val_loss: 0.0135\n",
      "Epoch 12/20\n",
      "37812/37812 [==============================] - 56s 1ms/sample - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 13/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 14/20\n",
      "37812/37812 [==============================] - 58s 2ms/sample - loss: 0.0125 - val_loss: 0.0131\n",
      "Epoch 15/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0124 - val_loss: 0.0130\n",
      "Epoch 16/20\n",
      "37812/37812 [==============================] - 56s 1ms/sample - loss: 0.0123 - val_loss: 0.0130\n",
      "Epoch 17/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0123 - val_loss: 0.0131\n",
      "Epoch 18/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0122 - val_loss: 0.0132\n",
      "Epoch 19/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0121 - val_loss: 0.0140\n",
      "Epoch 20/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0121 - val_loss: 0.0138\n",
      "Val loss: 0.013006379914192522\n",
      "Fold : 1\n",
      "Train on 37812 samples, validate on 3312 samples\n",
      "Epoch 1/20\n",
      "37812/37812 [==============================] - 71s 2ms/sample - loss: 0.0160 - val_loss: 0.0138\n",
      "Epoch 2/20\n",
      "37812/37812 [==============================] - 59s 2ms/sample - loss: 0.0142 - val_loss: 0.0136\n",
      "Epoch 3/20\n",
      "37812/37812 [==============================] - 56s 1ms/sample - loss: 0.0138 - val_loss: 0.0135\n",
      "Epoch 4/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0135 - val_loss: 0.0138\n",
      "Epoch 5/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0133 - val_loss: 0.0139\n",
      "Epoch 6/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0132 - val_loss: 0.0143\n",
      "Epoch 7/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 8/20\n",
      "37812/37812 [==============================] - 54s 1ms/sample - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 9/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0129 - val_loss: 0.0137\n",
      "Epoch 10/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0128 - val_loss: 0.0289\n",
      "Epoch 11/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0127 - val_loss: 0.0128\n",
      "Epoch 12/20\n",
      "37812/37812 [==============================] - 56s 1ms/sample - loss: 0.0126 - val_loss: 0.0127\n",
      "Epoch 13/20\n",
      "37812/37812 [==============================] - 57s 1ms/sample - loss: 0.0126 - val_loss: 0.0129\n",
      "Epoch 14/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0125 - val_loss: 0.0132\n",
      "Epoch 15/20\n",
      "37812/37812 [==============================] - 56s 1ms/sample - loss: 0.0124 - val_loss: 0.0131\n",
      "Epoch 16/20\n",
      "37812/37812 [==============================] - 56s 1ms/sample - loss: 0.0124 - val_loss: 0.0134\n",
      "Epoch 17/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0123 - val_loss: 0.0128\n",
      "Val loss: 0.012722931156385793\n",
      "Fold : 2\n",
      "Train on 37812 samples, validate on 3356 samples\n",
      "Epoch 1/20\n",
      "37812/37812 [==============================] - 63s 2ms/sample - loss: 0.0159 - val_loss: 0.0147\n",
      "Epoch 2/20\n",
      "37812/37812 [==============================] - 54s 1ms/sample - loss: 0.0142 - val_loss: 0.0139\n",
      "Epoch 3/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0139 - val_loss: 0.0139\n",
      "Epoch 4/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0137 - val_loss: 0.0135\n",
      "Epoch 5/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0134 - val_loss: 0.0410\n",
      "Epoch 6/20\n",
      "37812/37812 [==============================] - 70s 2ms/sample - loss: 0.0132 - val_loss: 0.0139\n",
      "Epoch 7/20\n",
      "37812/37812 [==============================] - 62s 2ms/sample - loss: 0.0131 - val_loss: 0.0132\n",
      "Epoch 8/20\n",
      "37812/37812 [==============================] - 65s 2ms/sample - loss: 0.0130 - val_loss: 0.0130\n",
      "Epoch 9/20\n",
      "37812/37812 [==============================] - 64s 2ms/sample - loss: 0.0129 - val_loss: 0.0134\n",
      "Epoch 10/20\n",
      "37812/37812 [==============================] - 73s 2ms/sample - loss: 0.0128 - val_loss: 0.0128\n",
      "Epoch 11/20\n",
      "37812/37812 [==============================] - 63s 2ms/sample - loss: 0.0127 - val_loss: 0.0131\n",
      "Epoch 12/20\n",
      "37812/37812 [==============================] - 63s 2ms/sample - loss: 0.0126 - val_loss: 0.0133\n",
      "Epoch 13/20\n",
      "37812/37812 [==============================] - 60s 2ms/sample - loss: 0.0125 - val_loss: 0.0129\n",
      "Epoch 14/20\n",
      "37812/37812 [==============================] - 66s 2ms/sample - loss: 0.0125 - val_loss: 0.0128\n",
      "Epoch 15/20\n",
      "37812/37812 [==============================] - 67s 2ms/sample - loss: 0.0124 - val_loss: 0.0128\n",
      "Epoch 16/20\n",
      "37812/37812 [==============================] - 68s 2ms/sample - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 17/20\n",
      "37812/37812 [==============================] - 68s 2ms/sample - loss: 0.0123 - val_loss: 0.0132\n",
      "Epoch 18/20\n",
      "37812/37812 [==============================] - 69s 2ms/sample - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 19/20\n",
      "37812/37812 [==============================] - 59s 2ms/sample - loss: 0.0122 - val_loss: 0.0150\n",
      "Epoch 20/20\n",
      "37812/37812 [==============================] - 62s 2ms/sample - loss: 0.0121 - val_loss: 0.0131\n",
      "Val loss: 0.012756625120349962\n",
      "Fold : 3\n",
      "Train on 37812 samples, validate on 3340 samples\n",
      "Epoch 1/20\n",
      "37812/37812 [==============================] - 64s 2ms/sample - loss: 0.0161 - val_loss: 0.0153\n",
      "Epoch 2/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0142 - val_loss: 0.0152\n",
      "Epoch 3/20\n",
      "37812/37812 [==============================] - 59s 2ms/sample - loss: 0.0138 - val_loss: 0.0144\n",
      "Epoch 4/20\n",
      "37812/37812 [==============================] - 54s 1ms/sample - loss: 0.0135 - val_loss: 0.0144\n",
      "Epoch 5/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0133 - val_loss: 0.0143\n",
      "Epoch 6/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0131 - val_loss: 0.0140\n",
      "Epoch 7/20\n",
      "37812/37812 [==============================] - 57s 1ms/sample - loss: 0.0130 - val_loss: 0.0141\n",
      "Epoch 8/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0129 - val_loss: 0.0139\n",
      "Epoch 9/20\n",
      "37812/37812 [==============================] - 56s 1ms/sample - loss: 0.0128 - val_loss: 0.0137\n",
      "Epoch 10/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0128 - val_loss: 0.0136\n",
      "Epoch 11/20\n",
      "37812/37812 [==============================] - 59s 2ms/sample - loss: 0.0127 - val_loss: 0.0135\n",
      "Epoch 12/20\n",
      "37812/37812 [==============================] - 64s 2ms/sample - loss: 0.0127 - val_loss: 0.0137\n",
      "Epoch 13/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0126 - val_loss: 0.0137\n",
      "Epoch 14/20\n",
      "37812/37812 [==============================] - 58s 2ms/sample - loss: 0.0126 - val_loss: 0.0136\n",
      "Epoch 15/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0125 - val_loss: 0.0135\n",
      "Epoch 16/20\n",
      "37812/37812 [==============================] - 61s 2ms/sample - loss: 0.0125 - val_loss: 0.0137\n",
      "Epoch 17/20\n",
      "37812/37812 [==============================] - 67s 2ms/sample - loss: 0.0124 - val_loss: 0.0137\n",
      "Epoch 18/20\n",
      "37812/37812 [==============================] - 56s 1ms/sample - loss: 0.0124 - val_loss: 0.0140\n",
      "Epoch 19/20\n",
      "37812/37812 [==============================] - 58s 2ms/sample - loss: 0.0123 - val_loss: 0.0135\n",
      "Epoch 20/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0123 - val_loss: 0.0144\n",
      "Val loss: 0.013504550552632173\n",
      "Fold : 4\n",
      "Train on 37812 samples, validate on 3310 samples\n",
      "Epoch 1/20\n",
      "37812/37812 [==============================] - 79s 2ms/sample - loss: 0.0161 - val_loss: 0.0142\n",
      "Epoch 2/20\n",
      "37812/37812 [==============================] - 60s 2ms/sample - loss: 0.0141 - val_loss: 0.0139\n",
      "Epoch 3/20\n",
      "37812/37812 [==============================] - 56s 1ms/sample - loss: 0.0138 - val_loss: 0.0141\n",
      "Epoch 4/20\n",
      "37812/37812 [==============================] - 62s 2ms/sample - loss: 0.0135 - val_loss: 0.0148\n",
      "Epoch 5/20\n",
      "37812/37812 [==============================] - 61s 2ms/sample - loss: 0.0133 - val_loss: 0.0138\n",
      "Epoch 6/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0132 - val_loss: 0.0133\n",
      "Epoch 7/20\n",
      "37812/37812 [==============================] - 58s 2ms/sample - loss: 0.0131 - val_loss: 0.0142\n",
      "Epoch 8/20\n",
      "37812/37812 [==============================] - 56s 1ms/sample - loss: 0.0130 - val_loss: 0.0140\n",
      "Epoch 9/20\n",
      "37812/37812 [==============================] - 66s 2ms/sample - loss: 0.0129 - val_loss: 0.0136\n",
      "Epoch 10/20\n",
      "37812/37812 [==============================] - 66s 2ms/sample - loss: 0.0128 - val_loss: 0.0135\n",
      "Epoch 11/20\n",
      "37812/37812 [==============================] - 60s 2ms/sample - loss: 0.0127 - val_loss: 0.0136\n",
      "Val loss: 0.01326929296928373\n",
      "Fold : 5\n",
      "Train on 37812 samples, validate on 3391 samples\n",
      "Epoch 1/20\n",
      "37812/37812 [==============================] - 64s 2ms/sample - loss: 0.0161 - val_loss: 0.0141\n",
      "Epoch 2/20\n",
      "37812/37812 [==============================] - 56s 1ms/sample - loss: 0.0142 - val_loss: 0.0135\n",
      "Epoch 3/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0138 - val_loss: 0.0134\n",
      "Epoch 4/20\n",
      "37812/37812 [==============================] - 59s 2ms/sample - loss: 0.0134 - val_loss: 0.0131\n",
      "Epoch 5/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0132 - val_loss: 0.0135\n",
      "Epoch 6/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0131 - val_loss: 0.0130\n",
      "Epoch 7/20\n",
      "37812/37812 [==============================] - 60s 2ms/sample - loss: 0.0130 - val_loss: 0.0131\n",
      "Epoch 8/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0129 - val_loss: 0.0127\n",
      "Epoch 9/20\n",
      "37812/37812 [==============================] - 57s 1ms/sample - loss: 0.0128 - val_loss: 0.0127\n",
      "Epoch 10/20\n",
      "37812/37812 [==============================] - 55s 1ms/sample - loss: 0.0128 - val_loss: 0.0128\n",
      "Epoch 11/20\n",
      "37812/37812 [==============================] - 60s 2ms/sample - loss: 0.0127 - val_loss: 0.0128\n",
      "Epoch 12/20\n",
      "37812/37812 [==============================] - 62s 2ms/sample - loss: 0.0126 - val_loss: 0.0130\n",
      "Epoch 13/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0126 - val_loss: 0.0127\n",
      "Epoch 14/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0125 - val_loss: 0.0128\n",
      "Epoch 15/20\n",
      "37812/37812 [==============================] - 52s 1ms/sample - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 16/20\n",
      "37812/37812 [==============================] - 52s 1ms/sample - loss: 0.0124 - val_loss: 0.0130\n",
      "Epoch 17/20\n",
      "37812/37812 [==============================] - 52s 1ms/sample - loss: 0.0123 - val_loss: 0.0131\n",
      "Epoch 18/20\n",
      "37812/37812 [==============================] - 57s 2ms/sample - loss: 0.0123 - val_loss: 0.0135\n",
      "Val loss: 0.01269266562834817\n",
      "Fold : 6\n",
      "Train on 37813 samples, validate on 3315 samples\n",
      "Epoch 1/20\n",
      "37813/37813 [==============================] - 63s 2ms/sample - loss: 0.0160 - val_loss: 0.0146\n",
      "Epoch 2/20\n",
      "37813/37813 [==============================] - 58s 2ms/sample - loss: 0.0141 - val_loss: 0.0143\n",
      "Epoch 3/20\n",
      "37813/37813 [==============================] - 56s 1ms/sample - loss: 0.0137 - val_loss: 0.0142\n",
      "Epoch 4/20\n",
      "37813/37813 [==============================] - 56s 1ms/sample - loss: 0.0134 - val_loss: 0.0146\n",
      "Epoch 5/20\n",
      "37813/37813 [==============================] - 56s 1ms/sample - loss: 0.0132 - val_loss: 0.0138\n",
      "Epoch 6/20\n",
      "37813/37813 [==============================] - 58s 2ms/sample - loss: 0.0131 - val_loss: 0.0154\n",
      "Epoch 7/20\n",
      "37813/37813 [==============================] - 57s 2ms/sample - loss: 0.0130 - val_loss: 0.0137\n",
      "Epoch 8/20\n",
      "37813/37813 [==============================] - 58s 2ms/sample - loss: 0.0129 - val_loss: 0.0141\n",
      "Epoch 9/20\n",
      "37813/37813 [==============================] - 58s 2ms/sample - loss: 0.0128 - val_loss: 0.0136\n",
      "Epoch 10/20\n",
      "37813/37813 [==============================] - 54s 1ms/sample - loss: 0.0128 - val_loss: 0.0136\n",
      "Epoch 11/20\n",
      "37813/37813 [==============================] - 55s 1ms/sample - loss: 0.0127 - val_loss: 0.0136\n",
      "Epoch 12/20\n",
      "37813/37813 [==============================] - 55s 1ms/sample - loss: 0.0126 - val_loss: 0.0142\n",
      "Epoch 13/20\n",
      "37813/37813 [==============================] - 57s 2ms/sample - loss: 0.0125 - val_loss: 0.0136\n",
      "Epoch 14/20\n",
      "37813/37813 [==============================] - 62s 2ms/sample - loss: 0.0125 - val_loss: 0.0138\n",
      "Epoch 15/20\n",
      "37813/37813 [==============================] - 59s 2ms/sample - loss: 0.0124 - val_loss: 0.0137\n",
      "Val loss: 0.013555054922046127\n",
      "Fold : 7\n",
      "Train on 37813 samples, validate on 3344 samples\n",
      "Epoch 1/20\n",
      "37813/37813 [==============================] - 80s 2ms/sample - loss: 0.0161 - val_loss: 0.0143\n",
      "Epoch 2/20\n",
      "37813/37813 [==============================] - 59s 2ms/sample - loss: 0.0141 - val_loss: 0.0141\n",
      "Epoch 3/20\n",
      "37813/37813 [==============================] - 59s 2ms/sample - loss: 0.0137 - val_loss: 0.0137\n",
      "Epoch 4/20\n",
      "37813/37813 [==============================] - 55s 1ms/sample - loss: 0.0135 - val_loss: 0.0138\n",
      "Epoch 5/20\n",
      "37813/37813 [==============================] - 56s 1ms/sample - loss: 0.0133 - val_loss: 0.0137\n",
      "Epoch 6/20\n",
      "37813/37813 [==============================] - 57s 2ms/sample - loss: 0.0131 - val_loss: 0.0135\n",
      "Epoch 7/20\n",
      "37813/37813 [==============================] - 56s 1ms/sample - loss: 0.0130 - val_loss: 0.0134\n",
      "Epoch 8/20\n",
      "37813/37813 [==============================] - 56s 1ms/sample - loss: 0.0129 - val_loss: 0.0137\n",
      "Epoch 9/20\n",
      "37813/37813 [==============================] - 56s 1ms/sample - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 10/20\n",
      "37813/37813 [==============================] - 54s 1ms/sample - loss: 0.0127 - val_loss: 0.0132\n",
      "Epoch 11/20\n",
      "37813/37813 [==============================] - 55s 1ms/sample - loss: 0.0127 - val_loss: 0.0141\n",
      "Epoch 12/20\n",
      "37813/37813 [==============================] - 56s 1ms/sample - loss: 0.0126 - val_loss: 0.0135\n",
      "Epoch 13/20\n",
      "37813/37813 [==============================] - 55s 1ms/sample - loss: 0.0125 - val_loss: 0.0140\n",
      "Epoch 14/20\n",
      "37813/37813 [==============================] - 55s 1ms/sample - loss: 0.0125 - val_loss: 0.0132\n",
      "Epoch 15/20\n",
      "37813/37813 [==============================] - 57s 1ms/sample - loss: 0.0124 - val_loss: 0.0132\n",
      "Val loss: 0.013163345064324313\n",
      "0.01308385566594535\n",
      "CPU times: user 6h 35min 31s, sys: 5h 48min 52s, total: 12h 24min 23s\n",
      "Wall time: 2h 21min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "models = []\n",
    "kf = KFold(n_splits=8, shuffle=True, random_state=42)\n",
    "score = []\n",
    "\n",
    "for i, (tdx, vdx) in enumerate(kf.split(train_x, train_y)):\n",
    "    print(f'Fold : {i}')\n",
    "    X_train, X_val = train_x[tdx], train_x[vdx],\n",
    "    y_train, y_val = train_y.iloc[tdx]['YardIndexClipped'].values, train_y.iloc[vdx]['YardIndexClipped'].values\n",
    "    season_val = df_season.iloc[vdx]['Season'].values\n",
    "\n",
    "    y_train_values = np.zeros((len(y_train), num_classes_y), np.int32)\n",
    "    for irow, row in enumerate(y_train):\n",
    "        y_train_values[(irow, row - min_idx_y)] = 1\n",
    "        \n",
    "    y_val_values = np.zeros((len(y_val), num_classes_y), np.int32)\n",
    "    for irow, row in enumerate(y_val - min_idx_y):\n",
    "        y_val_values[(irow, row)] = 1\n",
    "\n",
    "    val_idx = np.where(season_val!=2017)\n",
    "    \n",
    "    X_val = X_val[val_idx]\n",
    "    y_val_values = y_val_values[val_idx]\n",
    "\n",
    "    y_train_values = y_train_values.astype('float32')\n",
    "    y_val_values = y_val_values.astype('float32')\n",
    "    \n",
    "    model = get_conv_net(num_classes_y)\n",
    "\n",
    "    es = EarlyStopping(monitor='val_CRPS',\n",
    "                        mode='min',\n",
    "                        restore_best_weights=True,\n",
    "                        verbose=0,\n",
    "                        patience=10)\n",
    "    \n",
    "    es.set_model(model)\n",
    "    metric = Metric(model, [es], [X_val, y_val_values])\n",
    "\n",
    "    n_epochs = 30 \n",
    "    \n",
    "    opt = Adam(learning_rate=1e-3)\n",
    "    model.compile(loss=crps,\n",
    "                  optimizer=opt)\n",
    "    \n",
    "    model.fit(X_train,\n",
    "              y_train_values, \n",
    "              epochs=n_epochs,\n",
    "              batch_size=64,\n",
    "              verbose=1,\n",
    "              callbacks=[metric],\n",
    "              validation_data=(X_val, y_val_values))\n",
    "\n",
    "    val_crps_score = min(model.history.history['val_CRPS'])\n",
    "    print(\"Val loss: {}\".format(val_crps_score))\n",
    "    \n",
    "    score.append(val_crps_score)\n",
    "\n",
    "    models.append(model)\n",
    "    \n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean validation loss is {}\".format(np.mean(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is not possible to be submitted in kaggle because it does not satisfy the code requirements (CPU time less than 4 hours). Furthermore, it can be seen that we are adding complexity to the problem since we are converting tabular data to images. \n",
    "\n",
    "However, this first attempt shows that the player influence area concept is useful to predict the outcome of a rushing play, since we should reach the 71st place in the competition according to the CV score. Also, it can be improved in several ways:\n",
    "- Improve the convolutional network architecture. The architecture proposes in this notebook has not been tunned.\n",
    "- Avoid some unnecessary calculus in the player influence area. We are computing the player influence area for some points that are far away from the player and the result can be approximated as zero.\n",
    "- Use a speed max based on the NFL data. The speed max is a parameter used to compute the player influence area. In this first attempt, we use a soccer speed max\n",
    "\n",
    "With this first attempt, we aim to encourage other competitors to exploit the concept of the **_player influence area_** to predict the outcome of a rushing play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://github.com/juancamilocampos/nfl-big-data-bowl-2020/blob/master/1st_place_zoo_solution_v2.ipynb\n",
    "\n",
    "[2] Fernandez Javier, and Bornn Luke (2018). Wide Open Spaces: A statistical technique for measuring space creation in professional soccer.\n",
    "\n",
    "[3] https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/125010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('kaggle_competitions': conda)",
   "language": "python",
   "name": "python38264bitkagglecompetitionscondac8f386bd51fb43ad9ebc42294a553a7c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
